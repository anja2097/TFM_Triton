{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## En caso de no tener las librerías necesarias instaladas"
      ],
      "metadata": {
        "id": "Pma305HVxVen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "09Zt1HTgxVk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "bWLJ-TxanKlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TRITON_PRINT_AUTOTUNING\"] = \"1\"\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import time\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import itertools\n",
        "from statistics import mean, median\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "j9QYIc2RnJTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración autotuning"
      ],
      "metadata": {
        "id": "HiVu9qPMwH3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Comentar/Descomentar en funcion del tipo de autotuning deseado (grid search (1ª opcion) o pre-configuraciones (2ª opcion))\n",
        "\n",
        "#Grid search\n",
        "'''\n",
        "# Rango de valores a explorar\n",
        "block_size_ms = [32, 64, 128,256]\n",
        "block_size_ns = [32, 64, 128,256]\n",
        "block_size_ks = [16, 32, 64, 128, 256]\n",
        "num_warps_list = [4]\n",
        "num_stages_list = [2]\n",
        "group_sizes = [8]\n",
        "\n",
        "# Generar todas las combinaciones posibles como un grid search\n",
        "\n",
        "autotune_configs = [\n",
        "    triton.Config(\n",
        "        {\n",
        "            \"BLOCK_SIZE_M\": bm,\n",
        "            \"BLOCK_SIZE_N\": bn,\n",
        "            \"BLOCK_SIZE_K\": bk,\n",
        "            \"GROUP_SIZE\": g\n",
        "        },\n",
        "        num_warps=w,\n",
        "        num_stages=s\n",
        "    )\n",
        "    for bm, bn, bk, g, w, s in itertools.product(\n",
        "        block_size_ms, block_size_ns, block_size_ks,\n",
        "        group_sizes, num_warps_list, num_stages_list\n",
        "    )\n",
        "]\n",
        "'''\n",
        "\n",
        "#Pre-configs\n",
        "autotune_configs = [\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 64, \"GROUP_SIZE\": 8}, num_stages=3, num_warps=8),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 64, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=4, num_warps=4),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 128, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=4, num_warps=4),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=4, num_warps=4),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 64, \"BLOCK_SIZE_N\": 128, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=4, num_warps=4),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 32, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=4, num_warps=4),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 64, \"BLOCK_SIZE_N\": 32, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=5, num_warps=2),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 32, \"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=5, num_warps=2),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 32, \"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=5, num_warps=2),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 32, \"BLOCK_SIZE_N\": 32, \"BLOCK_SIZE_K\": 16, \"GROUP_SIZE\": 4}, num_stages=4, num_warps=2),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 64, \"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": 16, \"GROUP_SIZE\": 8}, num_stages=3, num_warps=4),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 256, \"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=3, num_warps=8),\n",
        "]"
      ],
      "metadata": {
        "id": "c5oSsXElwH9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementaciones del kernel de GEMM"
      ],
      "metadata": {
        "id": "vu70XetPnNvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FP16"
      ],
      "metadata": {
        "id": "t1Kgx7cXUfY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(f'cuda:{0}') #Seleccionamos la GPU sobre la que se ejecuta el codigo\n",
        "torch.manual_seed(0) #Para obtener siempre los mismos resultados\n",
        "\n",
        "#Kernel Triton\n",
        "@triton.autotune(configs=autotune_configs, key=[\"M\", \"N\", \"K\"])\n",
        "@triton.jit\n",
        "def matmul_kernel(\n",
        "    A_ptr,\n",
        "    B_ptr,\n",
        "    C_ptr,\n",
        "    M,\n",
        "    N,\n",
        "    K,\n",
        "    stride_a_M,\n",
        "    stride_a_K,\n",
        "    stride_b_K,\n",
        "    stride_b_N,\n",
        "    stride_c_M,\n",
        "    stride_c_N,\n",
        "    BLOCK_SIZE_M: tl.constexpr,\n",
        "    BLOCK_SIZE_N: tl.constexpr,\n",
        "    BLOCK_SIZE_K: tl.constexpr,\n",
        "    GROUP_SIZE: tl.constexpr,\n",
        "    num_stages: tl.constexpr,\n",
        "    num_warps: tl.constexpr,\n",
        "):\n",
        "\n",
        "    PID = tl.program_id(0)\n",
        "    num_PID_along_M = tl.cdiv(M, BLOCK_SIZE_M)\n",
        "    num_PID_along_N = tl.cdiv(N, BLOCK_SIZE_N)\n",
        "    num_PID_in_group = GROUP_SIZE * num_PID_along_N\n",
        "    group_id = PID // num_PID_in_group\n",
        "    first_PID_in_group_along_M = group_id * GROUP_SIZE\n",
        "    group_size_adj = min(num_PID_along_M - first_PID_in_group_along_M, GROUP_SIZE)\n",
        "    PID_M = first_PID_in_group_along_M + ((PID % num_PID_in_group) % group_size_adj)\n",
        "    PID_N = ((PID % num_PID_in_group) // group_size_adj)\n",
        "\n",
        "    offsets_M = PID_M * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
        "    offsets_N = PID_N * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
        "    offsets_K = tl.arange(0, BLOCK_SIZE_K)\n",
        "\n",
        "    a_offsets = offsets_M[:,None] * stride_a_M + offsets_K[None,:] * stride_a_K #[[m1n1],[m1n2],[m1,n3]] Resultados \"visuales\" de estas operaciones\n",
        "                                                                                #[[m2n1],[m2n2],[m2,n3]]\n",
        "                                                                                #[[m3n1],[m3n2],[m3,n3]]\n",
        "    b_offsets = offsets_K[:,None] * stride_b_K + offsets_N[None,:] * stride_b_N #[[k1n1],[k1n2],[k1,n3]]\n",
        "\n",
        "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
        "\n",
        "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
        "        mask = offsets_K < K - k * BLOCK_SIZE_K\n",
        "        a_c = tl.load(A_ptr + a_offsets, mask=mask[None,:],other=0.0)\n",
        "        b_c = tl.load(B_ptr + b_offsets, mask=mask[:,None],other=0.0)\n",
        "\n",
        "        accumulator = tl.dot(a_c,b_c,acc=accumulator) #tl.dot hace un matmul. Es como hacer a@b\n",
        "\n",
        "        a_offsets +=  BLOCK_SIZE_K * stride_a_K\n",
        "        b_offsets +=  BLOCK_SIZE_K * stride_b_K\n",
        "\n",
        "    c_offsets = offsets_M[:,None] * stride_c_M + offsets_N[None,:] * stride_c_N\n",
        "    c_mask = (offsets_M[:,None] < M) & (offsets_N[None,:] < N)\n",
        "    tl.store(C_ptr + c_offsets, accumulator.to(tl.float16), mask=c_mask)\n",
        "\n",
        "\n",
        "def matmul_tri(a,b,c,M,N,K,grid): #Funcion wrapper del kernel de triton\n",
        "    matmul_kernel[grid](a,b,c,\n",
        "                        M,N,K,\n",
        "                        a.stride(0),a.stride(1),\n",
        "                        b.stride(0),b.stride(1),\n",
        "                        c.stride(0),c.stride(1),)\n",
        "    return c\n",
        "\n",
        "\n",
        "def matmul_cu(a,b): #Funcion wrapper CuPy\n",
        "    c_cp = cp.matmul(a,b)\n",
        "    return c_cp\n",
        "\n",
        "def matmul_cb(a,b): #Funcion wrapper del kernel de torch\n",
        "    c_cb = torch.matmul(a,b)\n",
        "    return c_cb\n",
        "\n",
        "def ejecucion_capa(M,N,K,num_repeticiones=100):\n",
        "    tiempos_triton = []\n",
        "    tiempos_cupy = []\n",
        "    tiempos_cublas = []\n",
        "    #Tensores triton\n",
        "    a_tri = torch.rand(M, K, device=DEVICE, dtype=torch.float16)\n",
        "    b_tri = torch.rand(K, N, device=DEVICE, dtype=torch.float16)\n",
        "    c_tri = torch.rand(M, N, device=DEVICE, dtype=torch.float16)\n",
        "    grid = lambda meta: (triton.cdiv(M, meta['BLOCK_SIZE_M']) * triton.cdiv(N, meta['BLOCK_SIZE_N']),)\n",
        "\n",
        "    #Matrices CuPy\n",
        "    a_cp = cp.asarray(a_tri.cpu().numpy()).astype(cp.float16)\n",
        "    b_cp = cp.asarray(b_tri.cpu().numpy()).astype(cp.float16)\n",
        "\n",
        "    #Tensores cuBLAS (torch)\n",
        "    a_cb = a_tri.to(torch.float16)\n",
        "    b_cb = b_tri.to(torch.float16)\n",
        "\n",
        "    #Primera ejecución para la compilación\n",
        "    #Triton\n",
        "    torch.cuda.synchronize()\n",
        "    start_tri = time.time()\n",
        "    c_tri = matmul_tri(a_tri,b_tri,c_tri,M,N,K,grid)\n",
        "    torch.cuda.synchronize()\n",
        "    end_tri = time.time()\n",
        "    print(f\"Tiempo compilación Triton: {end_tri-start_tri:.6f} s\")\n",
        "    #CuPy\n",
        "    cp.cuda.Device(0).synchronize()\n",
        "    start_cp = time.time()\n",
        "    c_cp = matmul_cu(a_cp,b_cp)\n",
        "    cp.cuda.Device(0).synchronize()\n",
        "    end_cp = time.time()\n",
        "    print(f\"Tiempo compilación CuPy: {end_cp-start_cp:.6f} s\")\n",
        "    #cuBLAS\n",
        "    torch.cuda.synchronize()\n",
        "    start_cublas = time.time()\n",
        "    c_cb = matmul_cb(a_cb,b_cb)\n",
        "    torch.cuda.synchronize()\n",
        "    end_cublas = time.time()\n",
        "    print(f\"Tiempo compilacion cuBLAS: {end_cublas-start_cublas:.6f} s\")\n",
        "\n",
        "\n",
        "    #Comprobación de que la operación realizada es la misma.\n",
        "    c_cp = torch.tensor(cp.asnumpy(c_cp), device=DEVICE, dtype=torch.float16)\n",
        "    #Comprobacion con CuPy\n",
        "    torch.testing.assert_close(c_tri, c_cp, atol=1e-3, rtol=1e-3)\n",
        "    #Comprobacion con cuBLAS\n",
        "    torch.testing.assert_close(c_tri, c_cb, atol=1e-3, rtol=1e-3)\n",
        "\n",
        "    # Repeticiones para medición\n",
        "    for _ in range(num_repeticiones):\n",
        "        # Medir Triton\n",
        "        torch.cuda.synchronize()\n",
        "        start_tri = time.time()\n",
        "        c_tri = matmul_tri(a_tri, b_tri, c_tri, M, N, K, grid)\n",
        "        torch.cuda.synchronize()\n",
        "        end_tri = time.time()\n",
        "        tiempos_triton.append(end_tri - start_tri)\n",
        "\n",
        "        # Medir CuPy\n",
        "        cp.cuda.Device(0).synchronize()\n",
        "        start_cp = time.time()\n",
        "        c_cp = matmul_cu(a_cp, b_cp)\n",
        "        cp.cuda.Device(0).synchronize()\n",
        "        end_cp = time.time()\n",
        "        tiempos_cupy.append(end_cp - start_cp)\n",
        "\n",
        "        # Medir cuBLAS\n",
        "        torch.cuda.synchronize()\n",
        "        start_cublas = time.time()\n",
        "        c_cb = c_cublas = matmul_cb(a_cb,b_cb)\n",
        "        torch.cuda.synchronize()\n",
        "        end_cublas = time.time()\n",
        "        tiempos_cublas.append(end_cublas - start_cublas)\n",
        "\n",
        "    # Calcular estadísticas\n",
        "    stats_triton = {\n",
        "        'min': min(tiempos_triton),\n",
        "        'max': max(tiempos_triton),\n",
        "        'media': mean(tiempos_triton),\n",
        "        'mediana': median(tiempos_triton)\n",
        "    }\n",
        "\n",
        "    stats_cupy = {\n",
        "        'min': min(tiempos_cupy),\n",
        "        'max': max(tiempos_cupy),\n",
        "        'media': mean(tiempos_cupy),\n",
        "        'mediana': median(tiempos_cupy)\n",
        "    }\n",
        "\n",
        "    stats_cublas = {\n",
        "        'min': min(tiempos_cublas),\n",
        "        'max': max(tiempos_cublas),\n",
        "        'media': mean(tiempos_cublas),\n",
        "        'mediana': median(tiempos_cublas)\n",
        "    }\n",
        "\n",
        "    return stats_triton, stats_cupy, stats_cublas"
      ],
      "metadata": {
        "id": "Mpz_B_xkUfg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INT 8"
      ],
      "metadata": {
        "id": "mFAtJjGZ9tEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(f'cuda:{0}') #Seleccionamos la GPU sobre la que se ejecuta el codigo\n",
        "torch.manual_seed(0) #Para obtener siempre los mismos resultados\n",
        "\n",
        "#Kernel Triton\n",
        "@triton.autotune(configs=autotune_configs, key=[\"M\", \"N\", \"K\"])\n",
        "@triton.jit\n",
        "def matmul_kernel_int8(\n",
        "    A_ptr,\n",
        "    B_ptr,\n",
        "    C_ptr,\n",
        "    M,\n",
        "    N,\n",
        "    K,\n",
        "    stride_a_M,\n",
        "    stride_a_K,\n",
        "    stride_b_K,\n",
        "    stride_b_N,\n",
        "    stride_c_M,\n",
        "    stride_c_N,\n",
        "    BLOCK_SIZE_M: tl.constexpr,\n",
        "    BLOCK_SIZE_N: tl.constexpr,\n",
        "    BLOCK_SIZE_K: tl.constexpr,\n",
        "    GROUP_SIZE: tl.constexpr,\n",
        "    num_stages: tl.constexpr,\n",
        "    num_warps: tl.constexpr,\n",
        "):\n",
        "\n",
        "    PID = tl.program_id(0)\n",
        "    num_PID_along_M = tl.cdiv(M, BLOCK_SIZE_M)\n",
        "    num_PID_along_N = tl.cdiv(N, BLOCK_SIZE_N)\n",
        "    num_PID_in_group = GROUP_SIZE * num_PID_along_N\n",
        "    group_id = PID // num_PID_in_group\n",
        "    first_PID_in_group_along_M = group_id * GROUP_SIZE\n",
        "    group_size_adj = min(num_PID_along_M - first_PID_in_group_along_M, GROUP_SIZE)\n",
        "    PID_M = first_PID_in_group_along_M + ((PID % num_PID_in_group) % group_size_adj)\n",
        "    PID_N = ((PID % num_PID_in_group) // group_size_adj)\n",
        "\n",
        "    offsets_M = PID_M * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
        "    offsets_N = PID_N * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
        "    offsets_K = tl.arange(0, BLOCK_SIZE_K)\n",
        "\n",
        "    a_offsets = offsets_M[:,None] * stride_a_M + offsets_K[None,:] * stride_a_K #[[m1n1],[m1n2],[m1,n3]] Resultados \"visuales\" de estas operaciones\n",
        "                                                                                #[[m2n1],[m2n2],[m2,n3]]\n",
        "                                                                                #[[m3n1],[m3n2],[m3,n3]]\n",
        "    b_offsets = offsets_K[:,None] * stride_b_K + offsets_N[None,:] * stride_b_N #[[k1n1],[k1n2],[k1,n3]]\n",
        "\n",
        "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n",
        "\n",
        "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
        "        mask = offsets_K < K - k * BLOCK_SIZE_K\n",
        "        a_c = tl.load(A_ptr + a_offsets, mask=mask[None,:],other=0)\n",
        "        b_c = tl.load(B_ptr + b_offsets, mask=mask[:,None],other=0)\n",
        "\n",
        "        accumulator = tl.dot(a_c,b_c,acc=accumulator) #tl.dot hace un matmul. Es como hacer a@b\n",
        "\n",
        "        a_offsets +=  BLOCK_SIZE_K * stride_a_K\n",
        "        b_offsets +=  BLOCK_SIZE_K * stride_b_K\n",
        "\n",
        "    c_offsets = offsets_M[:,None] * stride_c_M + offsets_N[None,:] * stride_c_N\n",
        "    c_mask = (offsets_M[:,None] < M) & (offsets_N[None,:] < N)\n",
        "    tl.store(C_ptr + c_offsets, accumulator.to(tl.int8), mask=c_mask)\n",
        "\n",
        "\n",
        "def matmul_tri_int8(a,b,c,M,N,K,grid):\n",
        "    matmul_kernel_int8[grid](a,b,c,\n",
        "                        M,N,K,\n",
        "                        a.stride(0),a.stride(1),\n",
        "                        b.stride(0),b.stride(1),\n",
        "                        c.stride(0),c.stride(1),)\n",
        "    return c\n",
        "\n",
        "def matmul_cu(a,b): #Funcion wrapper CuPy\n",
        "    c_cp = cp.matmul(a,b)\n",
        "    return c_cp\n",
        "\n",
        "def ejecucion_capa_int8(M,N,K,num_repeticiones=100):\n",
        "    tiempos_triton = []\n",
        "    tiempos_cupy = []\n",
        "    tiempos_cublas = []\n",
        "    #Tensores triton\n",
        "    a_tri = torch.randint(-128, 128, (M, K), device=DEVICE, dtype=torch.int8)\n",
        "    b_tri = torch.randint(-128, 128, (K, N), device=DEVICE, dtype=torch.int8)\n",
        "    c_tri = torch.zeros(M, N, device=DEVICE, dtype=torch.int8)\n",
        "    grid = lambda meta: (triton.cdiv(M, meta['BLOCK_SIZE_M']) * triton.cdiv(N, meta['BLOCK_SIZE_N']),)\n",
        "\n",
        "    #Matrices CuPy\n",
        "    a_cp = cp.asarray(a_tri.cpu().numpy()).astype(cp.int8)\n",
        "    b_cp = cp.asarray(b_tri.cpu().numpy()).astype(cp.int8)\n",
        "\n",
        "    #Primera ejecución para la compilación\n",
        "    #Triton\n",
        "    torch.cuda.synchronize()\n",
        "    start_tri = time.time()\n",
        "    c_tri = matmul_tri_int8(a_tri,b_tri,c_tri,M,N,K,grid)\n",
        "    torch.cuda.synchronize()\n",
        "    end_tri = time.time()\n",
        "    print(f\"Tiempo compilación Triton: {end_tri-start_tri:.6f} s\")\n",
        "    #CuPy\n",
        "    cp.cuda.Device(0).synchronize()\n",
        "    start_cp = time.time()\n",
        "    c_cp = matmul_cu(a_cp,b_cp)\n",
        "    cp.cuda.Device(0).synchronize()\n",
        "    end_cp = time.time()\n",
        "    print(f\"Tiempo compilación CuPy: {end_cp-start_cp:.6f} s\")\n",
        "\n",
        "\n",
        "    #Comprobación de que la operación realizada es la misma.\n",
        "    c_cp = torch.tensor(cp.asnumpy(c_cp), device=DEVICE, dtype=torch.int8)\n",
        "    torch.testing.assert_close(c_tri, c_cp, atol=1e-2, rtol=0)\n",
        "    print(\"El resultado de la operacion que realizan es correcto\")\n",
        "\n",
        "    # Repeticiones para medición\n",
        "    for _ in range(num_repeticiones):\n",
        "        # Medir Triton\n",
        "        torch.cuda.synchronize()\n",
        "        start_tri = time.time()\n",
        "        c_tri = matmul_tri_int8(a_tri, b_tri, c_tri, M, N, K, grid)\n",
        "        torch.cuda.synchronize()\n",
        "        end_tri = time.time()\n",
        "        tiempos_triton.append(end_tri - start_tri)\n",
        "\n",
        "        # Medir CuPy\n",
        "        cp.cuda.Device(0).synchronize()\n",
        "        start_cp = time.time()\n",
        "        c_cp = matmul_cu(a_cp, b_cp)\n",
        "        cp.cuda.Device(0).synchronize()\n",
        "        end_cp = time.time()\n",
        "        tiempos_cupy.append(end_cp - start_cp)\n",
        "\n",
        "\n",
        "    # Calcular estadísticas\n",
        "    stats_triton = {\n",
        "        'min': min(tiempos_triton),\n",
        "        'max': max(tiempos_triton),\n",
        "        'media': mean(tiempos_triton),\n",
        "        'mediana': median(tiempos_triton)\n",
        "    }\n",
        "\n",
        "    stats_cupy = {\n",
        "        'min': min(tiempos_cupy),\n",
        "        'max': max(tiempos_cupy),\n",
        "        'media': mean(tiempos_cupy),\n",
        "        'mediana': median(tiempos_cupy)\n",
        "    }\n",
        "\n",
        "    return stats_triton, stats_cupy"
      ],
      "metadata": {
        "id": "jiU_Dwh19r4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FP32"
      ],
      "metadata": {
        "id": "8mGsdCaCyHSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(f'cuda:{0}')\n",
        "torch.manual_seed(0) #Para obtener siempre los mismos resultados\n",
        "\n",
        "autotune_configs = [\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 64, \"GROUP_SIZE\": 8}, num_stages=3, num_warps=8),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 64, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=4, num_warps=4),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 128, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=4, num_warps=4),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=4, num_warps=4),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 64, \"BLOCK_SIZE_N\": 128, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=4, num_warps=4),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 32, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=4, num_warps=4),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 64, \"BLOCK_SIZE_N\": 32, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=5, num_warps=2),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 32, \"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=5, num_warps=2),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 32, \"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=5, num_warps=2),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 32, \"BLOCK_SIZE_N\": 32, \"BLOCK_SIZE_K\": 16, \"GROUP_SIZE\": 4}, num_stages=4, num_warps=2),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 64, \"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": 16, \"GROUP_SIZE\": 8}, num_stages=3, num_warps=4),\n",
        "    triton.Config({\"BLOCK_SIZE_M\": 256, \"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE\": 8}, num_stages=3, num_warps=8),\n",
        "]\n",
        "\n",
        "@triton.autotune(configs=autotune_configs, key=[\"M\", \"N\", \"K\"])\n",
        "@triton.jit\n",
        "def matmul_kernel_fp32(\n",
        "    A_ptr,\n",
        "    B_ptr,\n",
        "    C_ptr,\n",
        "    M,\n",
        "    N,\n",
        "    K,\n",
        "    stride_a_M,\n",
        "    stride_a_K,\n",
        "    stride_b_K,\n",
        "    stride_b_N,\n",
        "    stride_c_M,\n",
        "    stride_c_N,\n",
        "    BLOCK_SIZE_M: tl.constexpr,\n",
        "    BLOCK_SIZE_N: tl.constexpr,\n",
        "    BLOCK_SIZE_K: tl.constexpr,\n",
        "    GROUP_SIZE: tl.constexpr,\n",
        "    num_stages: tl.constexpr,\n",
        "    num_warps: tl.constexpr,\n",
        "):\n",
        "\n",
        "    PID = tl.program_id(0)\n",
        "    num_PID_along_M = tl.cdiv(M, BLOCK_SIZE_M)\n",
        "    num_PID_along_N = tl.cdiv(N, BLOCK_SIZE_N)\n",
        "    num_PID_in_group = GROUP_SIZE * num_PID_along_N\n",
        "    group_id = PID // num_PID_in_group\n",
        "    first_PID_in_group_along_M = group_id * GROUP_SIZE\n",
        "    group_size_adj = min(num_PID_along_M - first_PID_in_group_along_M, GROUP_SIZE)\n",
        "    PID_M = first_PID_in_group_along_M + ((PID % num_PID_in_group) % group_size_adj)\n",
        "    PID_N = ((PID % num_PID_in_group) // group_size_adj)\n",
        "\n",
        "    offsets_M = PID_M * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
        "    offsets_N = PID_N * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
        "    offsets_K = tl.arange(0, BLOCK_SIZE_K)\n",
        "\n",
        "    a_offsets = offsets_M[:,None] * stride_a_M + offsets_K[None,:] * stride_a_K #[[m1n1],[m1n2],[m1,n3]]\n",
        "                                                                                #[[m2n1],[m2n2],[m2,n3]]\n",
        "                                                                                #[[m3n1],[m3n2],[m3,n3]]\n",
        "    b_offsets = offsets_K[:,None] * stride_b_K + offsets_N[None,:] * stride_b_N #[[k1n1],[k1n2],[k1,n3]]\n",
        "\n",
        "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
        "\n",
        "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
        "        mask = offsets_K < K - k * BLOCK_SIZE_K\n",
        "        a_c = tl.load(A_ptr + a_offsets, mask=mask[None,:],other=0.0)\n",
        "        b_c = tl.load(B_ptr + b_offsets, mask=mask[:,None],other=0.0)\n",
        "\n",
        "        accumulator = tl.dot(a_c,b_c,acc=accumulator) #tl.dot hace un matmul. Es como hacer a@b\n",
        "\n",
        "        a_offsets +=  BLOCK_SIZE_K * stride_a_K\n",
        "        b_offsets +=  BLOCK_SIZE_K * stride_b_K\n",
        "\n",
        "    c_offsets = offsets_M[:,None] * stride_c_M + offsets_N[None,:] * stride_c_N\n",
        "    c_mask = (offsets_M[:,None] < M) & (offsets_N[None,:] < N)\n",
        "    tl.store(C_ptr + c_offsets, accumulator, mask=c_mask)\n",
        "\n",
        "\n",
        "def matmul_tri_fp32(a,b,c,M,N,K,grid):\n",
        "    matmul_kernel_fp32[grid](a,b,c,\n",
        "                        M,N,K,\n",
        "                        a.stride(0),a.stride(1),\n",
        "                        b.stride(0),b.stride(1),\n",
        "                        c.stride(0),c.stride(1),)\n",
        "    return c\n",
        "\n",
        "\n",
        "def matmul_cu(a,b):\n",
        "    c_cp = cp.matmul(a,b)\n",
        "    return c_cp\n",
        "\n",
        "def matmul_cb(a,b):\n",
        "    c_cb = torch.matmul(a,b)\n",
        "    return c_cb\n",
        "\n",
        "def ejecucion_capa_fp32(M,N,K,num_repeticiones=100):\n",
        "    tiempos_triton = []\n",
        "    tiempos_cupy = []\n",
        "    tiempos_cublas = []\n",
        "    #Tensores triton\n",
        "    a_tri = torch.rand(M, K, device=DEVICE, dtype=torch.float32)\n",
        "    b_tri = torch.rand(K, N, device=DEVICE, dtype=torch.float32)\n",
        "    c_tri = torch.rand(M, N, device=DEVICE, dtype=torch.float32)\n",
        "    grid = lambda meta: (triton.cdiv(M, meta['BLOCK_SIZE_M']) * triton.cdiv(N, meta['BLOCK_SIZE_N']),)\n",
        "\n",
        "    #Matrices CuPy\n",
        "    a_cp = cp.asarray(a_tri.cpu().numpy()).astype(cp.float32)\n",
        "    b_cp = cp.asarray(b_tri.cpu().numpy()).astype(cp.float32)\n",
        "\n",
        "    #Tensores cuBLAS (torch)\n",
        "    a_cb = a_tri.to(torch.float32)\n",
        "    b_cb = b_tri.to(torch.float32)\n",
        "\n",
        "    #Primera ejecución para la compilación\n",
        "    #Triton\n",
        "    torch.cuda.synchronize()\n",
        "    start_tri = time.time()\n",
        "    c_tri = matmul_tri_fp32(a_tri,b_tri,c_tri,M,N,K,grid)\n",
        "    torch.cuda.synchronize()\n",
        "    end_tri = time.time()\n",
        "    print(f\"Tiempo compilación Triton: {end_tri-start_tri:.6f} s\")\n",
        "    #CuPy\n",
        "    cp.cuda.Device(0).synchronize()\n",
        "    start_cp = time.time()\n",
        "    c_cp = matmul_cu(a_cp,b_cp)\n",
        "    cp.cuda.Device(0).synchronize()\n",
        "    end_cp = time.time()\n",
        "    print(f\"Tiempo compilación CuPy: {end_cp-start_cp:.6f} s\")\n",
        "    #cuBLAS\n",
        "    torch.cuda.synchronize()\n",
        "    start_cublas = time.time()\n",
        "    c_cb = matmul_cb(a_cb,b_cb)\n",
        "    torch.cuda.synchronize()\n",
        "    end_cublas = time.time()\n",
        "    print(f\"Tiempo compilacion cuBLAS: {end_cublas-start_cublas:.6f} s\")\n",
        "\n",
        "\n",
        "    #Comprobación de que la operación realizada es la misma.\n",
        "    c_cp = torch.tensor(cp.asnumpy(c_cp), device=DEVICE, dtype=torch.float32)\n",
        "    #Comprobacion con CuPy\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.testing.assert_close(c_tri, c_cp, atol=1e-3, rtol=1e-3)\n",
        "    #Comprobacion con cuBLAS\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.testing.assert_close(c_tri, c_cb, atol=1e-3, rtol=1e-3)\n",
        "\n",
        "    # Repeticiones para medición\n",
        "    for _ in range(num_repeticiones):\n",
        "        # Medir Triton\n",
        "        torch.cuda.synchronize()\n",
        "        start_tri = time.time()\n",
        "        c_tri = matmul_tri_fp32(a_tri, b_tri, c_tri, M, N, K, grid)\n",
        "        torch.cuda.synchronize()\n",
        "        end_tri = time.time()\n",
        "        tiempos_triton.append(end_tri - start_tri)\n",
        "\n",
        "        # Medir CuPy\n",
        "        cp.cuda.Device(0).synchronize()\n",
        "        start_cp = time.time()\n",
        "        c_cp = matmul_cu(a_cp, b_cp)\n",
        "        cp.cuda.Device(0).synchronize()\n",
        "        end_cp = time.time()\n",
        "        tiempos_cupy.append(end_cp - start_cp)\n",
        "\n",
        "        # Medir cuBLAS\n",
        "        torch.cuda.synchronize()\n",
        "        start_cublas = time.time()\n",
        "        c_cb = c_cublas = matmul_cb(a_cb,b_cb)\n",
        "        torch.cuda.synchronize()\n",
        "        end_cublas = time.time()\n",
        "        tiempos_cublas.append(end_cublas - start_cublas)\n",
        "\n",
        "    # Calcular estadísticas\n",
        "    stats_triton = {\n",
        "        'min': min(tiempos_triton),\n",
        "        'max': max(tiempos_triton),\n",
        "        'media': mean(tiempos_triton),\n",
        "        'mediana': median(tiempos_triton)\n",
        "    }\n",
        "\n",
        "    stats_cupy = {\n",
        "        'min': min(tiempos_cupy),\n",
        "        'max': max(tiempos_cupy),\n",
        "        'media': mean(tiempos_cupy),\n",
        "        'mediana': median(tiempos_cupy)\n",
        "    }\n",
        "\n",
        "    stats_cublas = {\n",
        "        'min': min(tiempos_cublas),\n",
        "        'max': max(tiempos_cublas),\n",
        "        'media': mean(tiempos_cublas),\n",
        "        'mediana': median(tiempos_cublas)\n",
        "    }\n",
        "\n",
        "    return stats_triton, stats_cupy, stats_cublas"
      ],
      "metadata": {
        "id": "DnnCkbRsxOTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creamos los diccionaros correspondientes a los modelos (VGG16 y ResNET50 v 1.5)"
      ],
      "metadata": {
        "id": "TU67EdQm2zLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VGG16_capas = {\n",
        "    1:   [[\"01\"], 50176, 64, 27],\n",
        "    2:   [[\"03\"], 50176, 64, 576],\n",
        "    3:   [[\"06\"], 12544, 128, 576],\n",
        "    4:   [[\"08\"], 12544, 128, 1152],\n",
        "    5:   [[\"11\"], 3136, 256, 1152],\n",
        "    6:   [[\"13\", \"15\"], 3136, 256, 2304],\n",
        "    7:   [[\"18\"], 784, 256, 2304],\n",
        "    8:   [[\"20\", \"22\"], 784, 512, 4608],\n",
        "    9:   [[\"25\", \"27\", \"29\"], 196, 512, 4608]\n",
        "}\n",
        "\n",
        "ResNET50_capas = {\n",
        "    1:   [[\"001\"], 12544, 64, 147],\n",
        "    2:   [[\"006\"], 3136, 64, 64],\n",
        "    3:   [[\"009\", \"021\", \"031\"], 3136, 64, 576],\n",
        "    4:   [[\"012\", \"014\", \"024\", \"034\"], 3136, 256, 64],\n",
        "    5:   [[\"018\", \"028\"], 3136, 64, 256],\n",
        "    6:   [[\"038\"], 3136, 128, 256],\n",
        "    7:   [[\"041\", \"053\", \"063\", \"073\"], 784, 128, 1152],\n",
        "    8:   [[\"044\", \"056\", \"066\", \"076\"], 784, 512, 128],\n",
        "    9:   [[\"046\"], 784, 512, 256],\n",
        "    10:  [[\"050\", \"060\", \"070\"], 784, 128, 512],\n",
        "    11:  [[\"080\"], 784, 256, 512],\n",
        "    12:  [[\"083\", \"095\", \"105\", \"115\", \"125\", \"135\"], 196, 256, 2304],\n",
        "    13:  [[\"086\", \"098\", \"108\", \"118\", \"128\", \"138\"], 196, 1024, 256],\n",
        "    14:  [[\"088\"], 196, 1024, 512],\n",
        "    15:  [[\"092\", \"102\", \"112\", \"122\", \"132\"], 196, 256, 1024],\n",
        "    16:  [[\"142\"], 196, 512, 1024],\n",
        "    17:  [[\"145\", \"157\", \"167\"], 49, 512, 4608],\n",
        "    18:  [[\"148\", \"160\", \"170\"], 49, 2048, 512],\n",
        "    19:  [[\"150\"], 49, 2048, 1024],\n",
        "    20:  [[\"154\", \"164\"], 49, 512, 2048]\n",
        "}"
      ],
      "metadata": {
        "id": "jvaqs7AyxapW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creamos una función que facilite la ejecución de los modelos con distintos valores de parámetros"
      ],
      "metadata": {
        "id": "GgWR8TIMOjq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ejecutar_modelo(modelo,tipo_dato=\"fp16\",tamano_batch=1,num_repeticiones=100):\n",
        "  if tipo_dato==\"fp16\":\n",
        "    funcion = ejecucion_capa\n",
        "  elif tipo_dato==\"int8\":\n",
        "    funcion = ejecucion_capa_int8\n",
        "  else:\n",
        "    funcion = ejecucion_capa_fp32\n",
        "  resultados = {}\n",
        "  for layer_id, (num_capas, m, n, k) in modelo.items():\n",
        "      num_capas_str = \", \".join(num_capas)\n",
        "      print(\"-\" * 50)\n",
        "      print(f\"ID capa: {layer_id}\")\n",
        "      print(f\"Tamaño de matrices: m = {tamano_batch * m}, n = {n}, k = {k}\")\n",
        "      print(f\"Se emplea en las capas nº: {num_capas_str}\")\n",
        "\n",
        "      # Ejecutar mediciones\n",
        "      if tipo_dato==\"fp16\" or tipo_dato==\"fp32\":\n",
        "        stats_triton, stats_cupy, stats_cublas = funcion(tamano_batch * m, n, k, num_repeticiones)\n",
        "\n",
        "        # Guardar resultados\n",
        "        resultados[layer_id] = [\n",
        "                [tamano_batch * m, n, k],\n",
        "                [stats_triton['mediana'], stats_cupy['mediana'], stats_cublas['mediana']]\n",
        "            ]\n",
        "      elif tipo_dato==\"int8\":\n",
        "        stats_triton, stats_cupy = funcion(tamano_batch * m, n, k, num_repeticiones)\n",
        "\n",
        "        # Guardar resultados\n",
        "        resultados[layer_id] = [\n",
        "                [tamano_batch * m, n, k],\n",
        "                [stats_triton['mediana'], stats_cupy['mediana']]\n",
        "            ]\n",
        "\n",
        "      # Imprimir resultados\n",
        "      print(\"\\nEstadísticas de tiempo (segundos):\")\n",
        "      print(\"Triton:\")\n",
        "      print(f\"  Mínimo: {stats_triton['min']:.6f}\")\n",
        "      print(f\"  Máximo: {stats_triton['max']:.6f}\")\n",
        "      print(f\"  Media: {stats_triton['media']:.6f}\")\n",
        "      print(f\"  Mediana: {stats_triton['mediana']:.6f}\")\n",
        "\n",
        "      print(\"\\nCuPy:\")\n",
        "      print(f\"  Mínimo: {stats_cupy['min']:.6f}\")\n",
        "      print(f\"  Máximo: {stats_cupy['max']:.6f}\")\n",
        "      print(f\"  Media: {stats_cupy['media']:.6f}\")\n",
        "      print(f\"  Mediana: {stats_cupy['mediana']:.6f}\")\n",
        "      #Speedup\n",
        "      speedup_cupy = stats_cupy['mediana'] / stats_triton['mediana']\n",
        "      print(f\"\\nSpeedup (CuPy/Triton): {speedup_cupy:.2f}x\")\n",
        "      if tipo_dato==\"fp16\" or tipo_dato==\"fp32\":\n",
        "        print(\"\\ncuBLAS:\")\n",
        "        print(f\"  Mínimo: {stats_cublas['min']:.6f}\")\n",
        "        print(f\"  Máximo: {stats_cublas['max']:.6f}\")\n",
        "        print(f\"  Media: {stats_cublas['media']:.6f}\")\n",
        "        print(f\"  Mediana: {stats_cublas['mediana']:.6f}\")\n",
        "        #Speedup\n",
        "        speedup_cublas = stats_cublas['mediana'] / stats_triton['mediana']\n",
        "        print(f\"Speedup (cuBLAS/Triton): {speedup_cublas:.2f}x\")\n",
        "\n",
        "      print(\"-\" * 50)\n",
        "  return resultados"
      ],
      "metadata": {
        "id": "L6_qPLDHWOEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecución principal (Iteramos sobre las distintas capas de los modelos llamando a las GEMM creadas anteriormente)"
      ],
      "metadata": {
        "id": "iMrw__0U3sDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resultados = ejecutar_modelo(VGG16_capas,tipo_dato=\"fp16\",tamano_batch=1,num_repeticiones=100)"
      ],
      "metadata": {
        "id": "tRe1CmV1Y1fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Función para crear gráficos de resultado"
      ],
      "metadata": {
        "id": "XwdoQU3qnYHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_grafico(datos_capas, tipo, titulo, ruta_guardado):\n",
        "\n",
        "    # Calcular TFLOPS/TOPS para cada capa e implementación\n",
        "    capas = []\n",
        "    valores_triton = []\n",
        "    valores_cupy = []\n",
        "    valores_cublas = [] if tipo == 'fp' else None\n",
        "\n",
        "    for capa_id, datos in datos_capas.items():\n",
        "        m, n, k = datos[0]\n",
        "        if tipo == 'fp':\n",
        "            tiempo_triton, tiempo_cupy, tiempo_cublas = datos[1]\n",
        "        else:\n",
        "            tiempo_triton, tiempo_cupy = datos[1]\n",
        "\n",
        "        # operaciones TFLOPS/TIOPS\n",
        "        operaciones = lambda tiempo: (2 * m * n * k) / (tiempo * 1e12)\n",
        "\n",
        "        valores_triton.append(operaciones(tiempo_triton))\n",
        "        valores_cupy.append(operaciones(tiempo_cupy))\n",
        "        if tipo == 'fp':\n",
        "            valores_cublas.append(operaciones(tiempo_cublas))\n",
        "        capas.append(capa_id)\n",
        "\n",
        "    # Crear el gráfico\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    x = range(len(capas))\n",
        "\n",
        "    if tipo == 'fp':\n",
        "        width = 0.25\n",
        "        # Crear barras para FP\n",
        "        plt.bar([i - width for i in x], valores_triton, width, label='Triton', color='green')\n",
        "        plt.bar(x, valores_cupy, width, label='CuPy', color='red')\n",
        "        plt.bar([i + width for i in x], valores_cublas, width, label='cuBLAS', color='blue')\n",
        "    else:\n",
        "        width = 0.35\n",
        "        # Crear barras para INT\n",
        "        plt.bar([i - width/2 for i in x], valores_triton, width, label='Triton', color='green')\n",
        "        plt.bar([i + width/2 for i in x], valores_cupy, width, label='CuPy', color='red')\n",
        "\n",
        "    # Configurar el gráfico\n",
        "    plt.xlabel('# of layer')\n",
        "    plt.ylabel('TFLOPS' if tipo == 'fp' else 'TOPS')\n",
        "    plt.title(titulo)\n",
        "    plt.xticks(x, capas)\n",
        "    plt.legend()\n",
        "\n",
        "    # Añadir cuadrícula\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(ruta_guardado)\n",
        "    plt.close()\n",
        "\n",
        "    # Imprimir valores para verificación\n",
        "    print(\"\\nValores por capa:\")\n",
        "    for i, capa_id in enumerate(capas):\n",
        "        print(f\"\\nCapa {capa_id}:\")\n",
        "        print(f\"  Triton: {valores_triton[i]:.2f} {'TFLOPS' if tipo == 'fp' else 'TOPS'}\")\n",
        "        print(f\"  CuPy: {valores_cupy[i]:.2f} {'TFLOPS' if tipo == 'fp' else 'TOPS'}\")\n",
        "        if tipo == 'fp':\n",
        "            print(f\"  cuBLAS: {valores_cublas[i]:.2f} TFLOPS\")"
      ],
      "metadata": {
        "id": "HoMrdhtXnXiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardar el gráfico en función del resultado"
      ],
      "metadata": {
        "id": "7OK1LbuN5ztp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crear_grafico(resultados,'fp','VGG16 - FP16 - Triton vs CuPy vs cuBLAS', 'VGG16-COLAB-FP16-B1.png')"
      ],
      "metadata": {
        "id": "yM8fPeSWnvHq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}